# Production Data Drift & Retraining Workflow

## ğŸ¯ How Data Drift Works in Production

### Data Structure in GCS

```
gs://ml-model-bucket-22/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ tickets.csv              # Reference data (FROZEN snapshot from training)
â”‚   â”œâ”€â”€ new_tickets.csv          # New production data (accumulates over time)
â”‚   â””â”€â”€ combined_tickets.csv    # Reference + New (for retraining)
â”œâ”€â”€ ticket_urgency_model/
â”‚   â””â”€â”€ ticket_urgency_model.pkl # Trained model
â””â”€â”€ monitoring/
    â””â”€â”€ report_YYYYMMDD_HHMMSS.json  # Monitoring reports
```

### Key Concepts

1. **Reference Data (`tickets.csv`)**: 
   - **Frozen snapshot** of data used for initial training
   - **Never changes** - this is your baseline
   - Used to compare against new data for drift detection

2. **New Production Data (`new_tickets.csv`)**:
   - **Accumulates** over time as new tickets come in
   - Can be appended daily/weekly/monthly
   - Used to detect if data distribution changed

3. **Combined Data (`combined_tickets.csv`)**:
   - Reference + New data combined
   - Used for **retraining** the model
   - Ensures model learns from both old and new patterns

---

## ğŸ”„ Complete Workflow

### Step 1: Initial Setup (One-Time)

```bash
# Upload reference data (frozen snapshot)
python scripts/upload_data_to_gcs.py
# This uploads:
# - datasets/tickets.csv (reference)
# - datasets/new_tickets.csv (new)
# - datasets/combined_tickets.csv (combined)
```

### Step 2: Train Initial Model

```bash
# Trigger training DAG
# Uses: datasets/tickets.csv (or combined_tickets.csv if available)
# Output: ticket_urgency_model/ticket_urgency_model.pkl
```

### Step 3: Deploy Model

```bash
# GitHub Actions auto-deploys to Cloud Run
# Or manually: ./scripts/deploy_to_cloudrun.sh
```

### Step 4: Collect New Production Data

**In Production:**
- New tickets arrive daily
- Append to `datasets/new_tickets.csv` in GCS
- Or create daily files: `datasets/production/20251217.csv`

**For Testing:**
```bash
# Upload new data
python scripts/upload_data_to_gcs.py
# This will:
# 1. Upload reference (if not exists)
# 2. Upload new data
# 3. Combine reference + new â†’ combined_tickets.csv
```

### Step 5: Monitor for Drift

```bash
# Trigger monitoring DAG
# Compares:
# - Reference: datasets/tickets.csv
# - New: datasets/new_tickets.csv
# Result: Detects if distributions changed
```

### Step 6: Retrain Model (If Drift Detected)

```bash
# Trigger training DAG
# Now uses: datasets/combined_tickets.csv (reference + new)
# Output: New model uploaded to GCS
```

### Step 7: Reload Model in API

```bash
# Option A: Redeploy API (automatic via GitHub Actions)
git push origin main

# Option B: Use reload endpoint (if new model uploaded)
curl -X POST https://ticket-urgency-api-7j3n5753uq-el.a.run.app/reload-model
```

### Step 8: Test New Model

```bash
# Test prediction
curl -X POST https://ticket-urgency-api-7j3n5753uq-el.a.run.app/predict \
  -H "Content-Type: application/json" \
  -d '{"title": "Server down", "description": "Production server not responding", "source": "email", "customer_tier": "premium"}'
```

---

## ğŸ“‹ Why This Structure?

### Why Keep Reference Data Separate?

- **Baseline comparison**: Always compare new data against original training data
- **Drift detection**: See how much data has changed since training
- **Reproducibility**: Can always retrain from same baseline

### Why Combine for Retraining?

- **Learn new patterns**: Model adapts to recent data patterns
- **Maintain old patterns**: Still remembers original training patterns
- **Better performance**: Model works well on both old and new data

### Why Cache Model in API?

- **Performance**: Loading from disk is faster than downloading from GCS every request
- **Cost**: Fewer GCS API calls
- **Reliability**: Works even if GCS is temporarily unavailable

### Why Need Reload Endpoint?

- **Update without redeploy**: Don't need to rebuild Docker image
- **Quick model refresh**: After retraining, just reload
- **Testing**: Easy to test new models

---

## ğŸš€ Quick Test Workflow

### 1. Upload Data (Combines Reference + New)

```bash
python scripts/upload_data_to_gcs.py
```

**Output:**
- âœ… Reference: `datasets/tickets.csv`
- âœ… New: `datasets/new_tickets.csv`
- âœ… Combined: `datasets/combined_tickets.csv`

### 2. Check Drift

```bash
# Trigger monitoring DAG
# Compares reference vs new â†’ detects drift
```

### 3. Retrain (Uses Combined Data)

```bash
# Trigger training DAG
# Uses: datasets/combined_tickets.csv
# Output: New model in GCS
```

### 4. Reload Model in API

```bash
curl -X POST https://ticket-urgency-api-7j3n5753uq-el.a.run.app/reload-model
```

### 5. Test

```bash
curl -X POST https://ticket-urgency-api-7j3n5753uq-el.a.run.app/predict \
  -H "Content-Type: application/json" \
  -d '{"title": "Application running slow", "description": "Report export stuck", "source": "web", "customer_tier": "Gold"}'
```

---

## ğŸ” Understanding the Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reference Data  â”‚ (datasets/tickets.csv)
â”‚ (Frozen)        â”‚ â† Never changes, baseline for comparison
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Compare
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New Data        â”‚ (datasets/new_tickets.csv)
â”‚ (Accumulates)   â”‚ â† Grows over time
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Combine
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Combined Data   â”‚ (datasets/combined_tickets.csv)
â”‚ (For Retrain)   â”‚ â† Reference + New
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Train
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New Model       â”‚ (ticket_urgency_model/ticket_urgency_model.pkl)
â”‚ (In GCS)        â”‚ â† Uploaded after training
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Reload
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Model      â”‚ (In-memory cache)
â”‚ (Serving)      â”‚ â† Reloaded via /reload-model endpoint
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Summary

1. **Reference data** = Frozen baseline (never changes)
2. **New data** = Accumulates over time
3. **Combined data** = Reference + New (for retraining)
4. **Monitoring** = Compares Reference vs New (detects drift)
5. **Retraining** = Uses Combined data (learns from both)
6. **Reload** = Updates API cache without redeploy

**This is the standard production workflow!** âœ…
