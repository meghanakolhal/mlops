# MLOps Ticket Urgency Model Training Pipeline

End-to-end MLOps pipeline for training and deploying a ticket urgency classification model.

## ğŸ—ï¸ Architecture

- **Airflow**: Orchestrates daily model training workflows
- **MLflow**: Tracks experiments, metrics, and model versions
- **Google Cloud Storage (GCS)**: Stores trained models
- **Docker Compose**: Local development environment

## ğŸ“ Project Structure

Top-level layout (inside `airflow/`):

```
.
â”œâ”€â”€ dags/                         # Airflow DAG definitions
â”‚   â””â”€â”€ train_model.py            # Training DAG (calls scripts.train.main)
â”œâ”€â”€ scripts/                      # Training / deployment / monitoring scripts
â”‚   â”œâ”€â”€ train.py                  # Main training script (logs to MLflow, uploads model to GCS)
â”‚   â”œâ”€â”€ preprocess.py             # (Reserved for additional preprocessing logic)
â”‚   â”œâ”€â”€ deploy_to_cloudrun.sh     # Manual deploy of FastAPI service to Cloud Run
â”‚   â””â”€â”€ monitor_model.py          # Monitoring script (API health + basic data drift)
â”œâ”€â”€ api/                          # Model serving API (Cloud Run)
â”‚   â”œâ”€â”€ app.py                    # FastAPI app loading model from GCS
â”‚   â”œâ”€â”€ Dockerfile                # API container image (uvicorn + FastAPI)
â”‚   â””â”€â”€ requirements.txt          # API dependencies (FastAPI, sklearn, etc.)
â”œâ”€â”€ data/                         # Local data (used by training / monitoring)
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ tickets.csv           # Labeled tickets used for training
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml            # GitHub Actions CI/CD to build & deploy API to Cloud Run
â”œâ”€â”€ Dockerfile                    # Airflow image (scheduler + webserver)
â”œâ”€â”€ docker-compose.yaml           # Local stack (Airflow + PostgreSQL + MLflow)
â”œâ”€â”€ fernet.py                     # Local Fernet key helper (for Airflow if needed)
â”œâ”€â”€ README.md                     # Project overview (this file)
â”œâ”€â”€ DEPLOYMENT_ROADMAP.md         # End-to-end deployment & roadmap
â”œâ”€â”€ TEST_API.md                   # Examples for testing API endpoints
â”œâ”€â”€ GITHUB_SETUP_GUIDE.md         # How to configure GitHub Secrets & CI/CD
â”œâ”€â”€ QUICK_START.md                # Short quick-start and common commands
â”œâ”€â”€ SETUP_MAIN_BRANCH.md          # Notes on using main branch + CI
â”œâ”€â”€ DEBUG_CLOUD_RUN.md            # Notes for debugging Cloud Run issues
â”œâ”€â”€ FIX_ASGI_ERROR.md             # Why we use uvicorn (FastAPI ASGI)
â”œâ”€â”€ FIX_PERMISSIONS.md            # Cloud Run & IAM permission fixes
â””â”€â”€ FIX_VERSION_MISMATCH.md       # scikit-learn version mismatch explanation
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Google Cloud account with service account key
- GCS bucket for model storage

### Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/meghanakolhal/mlops.git
   cd mlops
   ```

2. **Place your service account key**:
   - Copy your GCP service account JSON key to `service-acc-key.json` in the root directory

3. **Start the services**:
   ```bash
   docker compose up -d --build
   ```

4. **Access Airflow UI**:
   - Open http://localhost:8085
   - Login: `admin` / `admin`

5. **Access MLflow UI**:
   - Open http://localhost:5000

### Running Training

**Option 1: Via Airflow UI**
- Trigger the `ticket_urgency_model_training` DAG from the Airflow UI

**Option 2: Manual execution**
```bash
docker compose exec airflow-scheduler bash -lc "MLFLOW_TRACKING_URI=http://mlflow:5000/ python /opt/airflow/scripts/train.py"
```

## ğŸ“Š Model Details

- **Algorithm**: Logistic Regression with TF-IDF + OneHotEncoder
- **Features**: 
  - Text: Title + Description (TF-IDF)
  - Categorical: Source, Customer Tier
- **Target**: Urgency classification (urgent/non-urgent)
- **Metrics Tracked**: Accuracy, Precision, Recall, F1-score

## ğŸ”„ Workflow

1. **Data Loading**: Reads from `data/raw/tickets.csv`
2. **Preprocessing**: Combines title + description, handles categorical features
3. **Training**: Trains logistic regression model
4. **Evaluation**: Computes metrics on train/validation/test sets
5. **Model Storage**: 
   - Saves locally to `models/ticket_urgency_model.pkl`
   - Uploads to GCS: `gs://ml-model-bucket-22/ticket_urgency_model/ticket_urgency_model.pkl`
6. **Tracking**: Logs metrics and parameters to MLflow

## ğŸ”§ Configuration

### Environment Variables

- `GOOGLE_APPLICATION_CREDENTIALS`: Path to GCP service account key
- `MLFLOW_TRACKING_URI`: MLflow server URL (default: `http://mlflow:5000/`)
- `GCS_BUCKET_NAME`: GCS bucket name (default: `ml-model-bucket-22`)

### GCS Configuration

Update bucket name in `scripts/train.py`:
```python
bucket_name = 'ml-model-bucket-22'  # Your bucket name
```

## ğŸ“ Notes

- Models are automatically uploaded to GCS after training
- MLflow tracks all experiments and metrics
- Airflow runs training daily (configurable schedule)
- Service account key is required for GCS access

## ğŸ” Security

- **Never commit** `service-acc-key.json` to git (already in `.gitignore`)
- Use environment variables for sensitive configuration in production
- Rotate service account keys regularly

## ğŸ“š Next Steps

- Model serving API (FastAPI/Flask)
- CI/CD pipeline for automated deployments
- Model monitoring and drift detection
- A/B testing framework

See `DEPLOYMENT_ROADMAP.md` for detailed deployment guide.

## ğŸ‘¤ Author

**Meghana Kolhal**
- GitHub: [@meghanakolhal](https://github.com/meghanakolhal)

## ğŸ“„ License

This project is part of an MLOps portfolio demonstration.

## Reloading Model After Retraining

After retraining the model, reload it in the API without redeploying:

**Windows (PowerShell/Git Bash):**
```bash
curl.exe -X POST https://ticket-urgency-api-7j3n5753uq-el.a.run.app/reload-model -H "Content-Length: 0"
```

**Linux/Mac:**
```bash
curl -X POST https://ticket-urgency-api-7j3n5753uq-el.a.run.app/reload-model -H "Content-Length: 0"
```

**Note:** Cloud Run requires a `Content-Length` header for POST requests. The `-H "Content-Length: 0"` flag satisfies this requirement.